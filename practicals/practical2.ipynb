{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 2: Generative and Discriminative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4)\n",
      "(120,)\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris['data'], iris['target']\n",
    "\n",
    "N, D = X.shape\n",
    "Ntrain = int(0.8 * N)\n",
    "shuffler = np.random.permutation(N)\n",
    "Xtrain = X[shuffler[:Ntrain]]\n",
    "ytrain = y[shuffler[:Ntrain]]\n",
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "Xtest = X[shuffler[Ntrain:]]\n",
    "ytest = y[shuffler[Ntrain:]]\n",
    "\n",
    "print(X, y)\n",
    "# print(y == 0)\n",
    "# print(X[y == 0].shape)\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (232, 16)\n",
      "y shape: (232,)\n",
      "[1. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1.\n",
      " 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 0.\n",
      " 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 0. 1.\n",
      " 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1.\n",
      " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "import _pickle as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "X, y = cp.load(open('voting.pickle', 'rb'))\n",
    "\n",
    "N, D = X.shape\n",
    "N_train = int(0.8 * N)\n",
    "N_test = N - N_train\n",
    "X_train = X[:N_train]\n",
    "y_train = y[:N_train]\n",
    "X_test = X[N_train:]\n",
    "y_test = y[N_train:]\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class NBC:\n",
    "    def __init__(self, feature_types, num_classes):\n",
    "        self.feature_types = feature_types\n",
    "        self.num_classes = num_classes\n",
    "        self.params = []\n",
    "        self.epsilon = 1e-6 # to avoid divide by zero\n",
    "        self.priors = []\n",
    "    \n",
    "    # create univariate Gaussian distributions with (mean, variance)\n",
    "    # set of distributions for EACH CLASS\n",
    "    def fit(self, Xtrain, ytrain):\n",
    "        for i in range(self.num_classes):\n",
    "            class_indicies = (ytrain == i)\n",
    "            # print(class_indicies.shape)\n",
    "            # print(X.shape)\n",
    "            X_of_class = Xtrain[class_indicies]\n",
    "            means = np.mean(X_of_class, axis=0)\n",
    "            # print(means)\n",
    "            stds = np.std(X_of_class, axis=0) + self.epsilon\n",
    "            # print(stds)\n",
    "            self.params.append((means, stds))\n",
    "            self.priors.append(np.mean(ytrain == i))\n",
    "    \n",
    "    def GaussianProb(means, stds, value):\n",
    "        return (1.0 / math.sqrt(2.0 * math.pi)) * np.exp((-0.5) * ((value - means)/stds)**2)\n",
    "    \n",
    "    # compute the class conditional probabilities for the new inputs on all classes\n",
    "    # then return the classes with the largest probability for each data point\n",
    "    def predict(self, Xtest):\n",
    "        class_probs = np.zeros((Xtest.shape[0], self.num_classes))\n",
    "\n",
    "        # calculate the probability that we would observe X given our parameters for each class\n",
    "        for i in range(self.num_classes):\n",
    "            param_means, param_stds = self.params[i]\n",
    "\n",
    "            # matrix same shape as Xtest\n",
    "            # meaning prob(this data comes from class i)\n",
    "            probs = NBC.GaussianProb(param_means, param_stds, Xtest)\n",
    "            # print(probs)\n",
    "            logprobs = np.log(probs)\n",
    "\n",
    "            # adding each row together -> meaning the prob row x's flower is class i\n",
    "            probClassI = np.sum(logprobs, axis=1) \n",
    "            # add the prior information\n",
    "            wpriors = probClassI + np.log(self.priors[i])\n",
    "\n",
    "            # print(wpriors)\n",
    "\n",
    "            # array with \n",
    "            # totals = np.sum(logprobs, axis=1) + np.log(self.priors[i])\n",
    "            # print(total)\n",
    "            class_probs[:,i] = wpriors\n",
    "\n",
    "        # print(class_probs)\n",
    "        return np.argmax(class_probs, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 4) (120,)\n",
      "(30, 4) (30,)\n",
      "Params: [(array([5.00909091, 3.42045455, 1.45      , 0.24545455]), array([0.36793021, 0.37994091, 0.16306819, 0.10325388])), (array([5.9       , 2.73714286, 4.16571429, 1.29428571]), array([0.54037124, 0.3439991 , 0.47143823, 0.19264109])), (array([6.57317073, 2.97804878, 5.56341463, 2.0195122 ]), array([0.61842717, 0.30724063, 0.51074226, 0.28475122]))]\n",
      "Priors: [0.36666666666666664, 0.2916666666666667, 0.3416666666666667]\n",
      "[2 2 1 2 1 1 1 2 0 1 2 1 1 1 1 0 1 2 0 1 1 1 2 2 0 2 0 1 0 1]\n",
      "0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "nbc = NBC(feature_types=['r','r','r','r'], num_classes=3)\n",
    "print(Xtrain.shape, ytrain.shape)\n",
    "print(Xtest.shape, ytest.shape)\n",
    "nbc.fit(Xtrain, ytrain)\n",
    "print(f\"Params: {nbc.params}\")\n",
    "print(f\"Priors: {nbc.priors}\")\n",
    "# print(Xtest)\n",
    "yhat = nbc.predict(Xtest)\n",
    "test_accuracy = np.mean(yhat == ytest)\n",
    "print(ytest)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logRegModel = LogisticRegression().fit(Xtrain, ytrain)\n",
    "logRegModel.score(Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handin 2: Finding the MSE (Mean Squared Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 5.878764675855028\n",
      "Average Sum Diffs Squared: train 0.7767772386501117 -- test 0.8138573000045535\n"
     ]
    }
   ],
   "source": [
    "# First find the mean\n",
    "meany = np.average(y_train)\n",
    "print(f\"Mean: {meany}\")\n",
    "\n",
    "def mse(predicted, expected):\n",
    "    diffs = predicted - expected\n",
    "    squared = np.square(diffs)\n",
    "    avg = np.average(squared)\n",
    "    return avg\n",
    "# then calculate the sum of squared differences from the mean\n",
    "# diffs_train = y_train - meany\n",
    "# diffs_test = y_test - meany\n",
    "# # print(f\"Diffs: train {diffs_train} -- test {diffs_test}\")\n",
    "# diffs_squared_train = np.square(diffs_train)\n",
    "# diffs_squared_test = np.square(diffs_test)\n",
    "# # print(f\"Diffs squared: train {diffs_squared_train} -- test {diffs_squared_test}\")\n",
    "# avg_diffs_squared_train = np.average(diffs_squared_train)\n",
    "# avg_diffs_squared_test = np.average(diffs_squared_test)\n",
    "print(f\"Average Sum Diffs Squared: train {mse(y_train, meany)} -- test {mse(y_test, meany)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handin 3: Standardizing Dataset and Finding Least Squares Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[[1. 2. 3. 1.]\n",
      " [4. 5. 6. 1.]\n",
      " [7. 8. 9. 1.]]\n"
     ]
    }
   ],
   "source": [
    "def find_standardize_params(matrix):\n",
    "    mean = np.average(matrix, axis=0)\n",
    "    std = np.std(matrix, axis=0)\n",
    "    return mean, std\n",
    "\n",
    "def standardize(mat, params):\n",
    "    mean, std = params\n",
    "    return (mat - mean) / std\n",
    "\n",
    "# add bias all ones to X's\n",
    "def addBias(matrix):\n",
    "    bias = np.ones((matrix.shape[0], 1))\n",
    "    return np.hstack((matrix, bias))\n",
    "\n",
    "\n",
    "test = np.arange(1,10).reshape((3,3))\n",
    "print(test)\n",
    "params = find_standardize_params(test)\n",
    "standardize(test, params)\n",
    "print(addBias(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: train 0.5639996173941925 -- test 0.5607292042283468\n"
     ]
    }
   ],
   "source": [
    "# meanX = np.average(X_train, axis=0)\n",
    "# print(meanX.shape)\n",
    "# stddev_X = np.std(X_train, axis=0)\n",
    "# stddev_y = np.std(y_train)\n",
    "\n",
    "# std_X_train = (X_train - meanX) / stddev_X\n",
    "# std_X_test = (X_test - meanX) / stddev_X\n",
    "# std_y_train = (X_train - meany) / stddev_y\n",
    "# std_y_test = (X_test - meany) / stddev_y\n",
    "\n",
    "# print(std_X_train)\n",
    "# print(std_y_train)\n",
    "\n",
    "# standardize all train and test data (using the train data for mean and std)\n",
    "stdparamsX = find_standardize_params(X_train)\n",
    "std_X_train = standardize(X_train, stdparamsX)\n",
    "std_X_test = standardize(X_test, stdparamsX)\n",
    "\n",
    "# stdparamsy = find_standardize_params(y_train)\n",
    "# std_y_train = standardize(y_train, stdparamsy)\n",
    "# std_y_test = standardize(y_test, stdparamsy)\n",
    "\n",
    "std_X_train_wbias = addBias(std_X_train)\n",
    "std_X_test_wbias = addBias(std_X_test)\n",
    "\n",
    "# least squares prediction\n",
    "# y_pred = Xw = X((X_TX)^-1)X_Ty\n",
    "# my mistake was standardizing the features X and response y when I don't have have to standardized y (response)\n",
    "w = (np.linalg.inv(std_X_train_wbias.transpose() @ std_X_train_wbias) @ std_X_train_wbias.transpose()) @ y_train\n",
    "y_pred_train = std_X_train_wbias @ w\n",
    "y_pred_test = std_X_test_wbias @ w\n",
    "\n",
    "# print(y_pred_train)\n",
    "\n",
    "print(f\"MSE: train {mse(y_pred_train, y_train)} -- test {mse(y_pred_test, y_test)}\")\n",
    "# print(np.mean(np.square(y_train - y_pred_train)))\n",
    "# print(np.mean(np.square(y_test - y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handin 4: Learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let us see if the linear model is overfitting or underfitting. Since the dataset is somewhat large\n",
    "and there are only 11 features, our guess should be that it may either be underfitting or be\n",
    "about right.\n",
    "Starting with 20 datapoints, weâ€™ll use training datasets of increasing size, in increments of 20,\n",
    "up to about 600 datapoints. For each case train the linear model only using the first n elements\n",
    "of the training data. Calculate the training error (on the data used) and the test error (on the\n",
    "full test set). Plot the training error and test error as a function of the size of the dataset used\n",
    "for training.\n",
    "Handin 4: Report the learning curves plot. Also, explain whether you think the model is\n",
    "underfitting or not, and how much data you need before getting the optimal test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABCJUlEQVR4nO3deXxU9aH///eZzGSyT1iyQtiEsosILhEFLC1UWitaq9W2yu39eS+94nK5tBa1rr3Gtt5et4q1trhX77e40EorKJtbRRCKQkDQQCIkBghkTyYzc35/nMyQyUYmZOYk8Ho+HucxZ535zIeQeefz+ZzPGKZpmgIAALCJw+4CAACAUxthBAAA2IowAgAAbEUYAQAAtiKMAAAAWxFGAACArQgjAADAVoQRAABgK6fdBeiKQCCgAwcOKDU1VYZh2F0cAADQBaZpqrq6Wrm5uXI4Om7/6BNh5MCBA8rLy7O7GAAAoBtKSko0ePDgDo/3iTCSmpoqyXozaWlpNpcGAAB0RVVVlfLy8kKf4x3pE2Ek2DWTlpZGGAEAoI853hALBrACAABbEUYAAICtCCMAAMBWfWLMCAAgdkzTlM/nk9/vt7so6OXi4uLkdDpPeNoNwggAIMTr9aq0tFR1dXV2FwV9RFJSknJychQfH9/t5yCMAAAkWRNMFhUVKS4uTrm5uYqPj2eiSXTINE15vV4dPHhQRUVFGjVqVKcTm3WGMAIAkGS1igQCAeXl5SkpKcnu4qAPSExMlMvl0r59++T1epWQkNCt52EAKwAgTHf/usWpqSd+XviJAwAAtiKMAADQwrBhw/Tggw92+fx169bJMAwdPXo0amU62TFmBADQp82cOVNnnHFGRAGiMx9++KGSk5O7fP55552n0tJSeTyeHnn9UxEtIwCAk15w7pSuyMjIiGgAb3x8vLKzs3vlnUder7fNPr/fr0AgEPFzdfe6rji1w8g/X5RW/kTa957dJQEAdMP8+fO1fv16PfTQQzIMQ4ZhaO/evaGukzfeeENTp06V2+3W22+/rc8++0yXXHKJsrKylJKSorPOOktvvvlm2HO27qYxDENPPvmkLr30UiUlJWnUqFFasWJF6HjrbpqnnnpK6enpeuONNzR27FilpKToG9/4hkpLS0PX+Hw+3XjjjUpPT9eAAQN0yy236Nprr9W8efM6fb/vvfeepk+frsTEROXl5enGG29UbW1tWNl/8YtfaP78+fJ4PLruuutC5fnrX/+qcePGye12a9++fTpy5IiuueYa9evXT0lJSbrooou0e/fu0HN1dF00RBRGCgoKdNZZZyk1NVWZmZmaN2+edu3a1ek1wX+k1svOnTtPqOA9YvcqaeMT0oGtdpcEAHod0zRV5/XZspim2aUyPvTQQ8rPz9d1112n0tJSlZaWKi8vL3T8pz/9qQoKClRYWKjTTz9dNTU1mjt3rt58801t2bJFc+bM0cUXX6zi4uJOX+fuu+/WFVdcoW3btmnu3Ln6/ve/r4qKig7Pr6ur0wMPPKBnn31WGzZsUHFxsRYvXhw6/stf/lLPP/+8li1bpnfffVdVVVV69dVXOy3Dxx9/rDlz5uiyyy7Ttm3b9NJLL+mdd97RwoULw8779a9/rQkTJmjz5s36+c9/HipPQUGBnnzySW3fvl2ZmZmaP3++Nm3apBUrVuj999+XaZqaO3eumpqawt5H6+uiIaIxI+vXr9f111+vs846Sz6fT7fddptmz56tHTt2HLd/bdeuXUpLSwttZ2RkdK/EPSmxn/XYcNTWYgBAb1Tf5Ne4O96w5bV33DNHSfHH/4jyeDyKj49XUlKSsrOz2xy/55579PWvfz20PWDAAE2aNCm0/Ytf/EKvvPKKVqxY0eZDvaX58+frqquukiTdd999euSRR7Rx40Z94xvfaPf8pqYmPf744zrttNMkSQsXLtQ999wTOv7II49oyZIluvTSSyVJjz76qFauXNnpe/31r3+tq6++WjfffLMkadSoUXr44Yc1Y8YMLV26NDTHx1e/+tWw4PPOO++oqalJjz32WOi97969WytWrNC7776r8847T5L0/PPPKy8vT6+++qq++93vht5Hy+uiJaIw8ve//z1se9myZcrMzNTmzZs1ffr0Tq/NzMxUenp6xAWMqoR067H+iK3FAABEx9SpU8O2a2trdffdd+uvf/2rDhw4IJ/Pp/r6+uO2jJx++umh9eTkZKWmpqq8vLzD85OSkkJBRJJycnJC51dWVurLL7/U2WefHToeFxenKVOmdDomY/PmzdqzZ4+ef/750D7TNEMz544dO7bd9yxZ41pavofCwkI5nU6dc845oX0DBgzQ6NGjVVhY2OF10XJCd9NUVlZKkvr373/ccydPnqyGhgaNGzdOt99+uy688MIOz21sbFRjY2Nou6qq6kSK2bFgywhhBADaSHTFacc9c2x77Z7QutX+Jz/5id544w098MADGjlypBITE3X55Ze3O9CzJZfLFbZtGEanwaG981t3PbUe8Hq8rqlAIKB///d/14033tjm2JAhQ0Lr7fVUJCYmhr1eR69lmmbYea2vi5ZuhxHTNLVo0SKdf/75mjBhQofn5eTk6IknntCUKVPU2NioZ599VrNmzdK6des6bE0pKCjQ3Xff3d2idR1hBAA6ZBhGl7pK7BYfH9/lbxh+++23NX/+/FD3SE1Njfbu3RvF0rXl8XiUlZWljRs36oILLpBk3amyZcsWnXHGGR1ed+aZZ2r79u0aOXLkCZdh3Lhx8vl8+uCDD0LdNIcPH9ann34aamGJpW7/lC1cuFDbtm3TO++80+l5o0eP1ujRo0Pb+fn5Kikp0QMPPNBhGFmyZIkWLVoU2q6qqgobkNRjQmHkaM8/NwAgJoYNG6YPPvhAe/fuVUpKSqet9SNHjtTLL7+siy++WIZh6Oc//3nUblftzA033KCCggKNHDlSY8aM0SOPPKIjR4502gpxyy236Nxzz9X111+v6667TsnJySosLNTq1av1yCOPRPT6o0aN0iWXXKLrrrtOv/vd75Samqqf/exnGjRokC655JITfXsR69atvTfccINWrFihtWvXavDgwRFff+6554bdPtSa2+1WWlpa2BIVienWIy0jANBnLV68WHFxcRo3bpwyMjI6Hf/xv//7v+rXr5/OO+88XXzxxZozZ47OPPPMGJbWcsstt+iqq67SNddco/z8fKWkpGjOnDmdftHc6aefrvXr12v37t264IILNHnyZP385z9XTk5Ot8qwbNkyTZkyRd/61reUn58v0zS1cuXKNl1MsWCYXb1/SlbXzA033KBXXnlF69at06hRo7r1opdffrkqKiq0Zs2aLp1fVVUlj8ejysrKng0m5YXSY+dKif2lW4p67nkBoA9qaGhQUVGRhg8f3u1vX0X3BAIBjR07VldccYXuvfdeu4sTkc5+brr6+R1RN83111+vF154Qa+99ppSU1NVVlYmyer/SkxMlGR1sezfv1/PPPOMJOnBBx/UsGHDNH78eHm9Xj333HNavny5li9fHtGbjYqWt/YGAhLfVAkAiIF9+/Zp1apVmjFjhhobG/Xoo4+qqKhIV199td1Fs0VEYWTp0qWSrO8BaGnZsmWaP3++JKm0tDSsiczr9Wrx4sXav3+/EhMTNX78eL3++uuaO3fuiZW8JwRv7TUDkrdaSuB7BQAA0edwOPTUU09p8eLFMk1TEyZM0JtvvmnL4NHeIKJuGrtErZtGkn6RLfnqpZv+KfUb1rPPDQB9CN006I6e6KahX4LbewEAsBVhhDACAICtCCOh23uP2lkKAABOWYQRWkYAALAVYYSJzwAAsBVhhJYRAABsRRgJzjXScNTOUgAAcMoijPBleQDQp82cOVM333xzjz7n/PnzNW/evB59TnSMMEI3DQAgRkzTlM/na7Pf6/V26/m6e11vQxihZQQA+qz58+dr/fr1euihh2QYhgzD0N69eyVJO3bs0Ny5c5WSkqKsrCz98Ic/1KFDh0LX/vnPf9bEiROVmJioAQMG6Gtf+5pqa2t111136emnn9Zrr70Wes5169a1+/qmaepXv/qVRowYocTERE2aNEl//vOfQ8fXrVsnwzD0xhtvaOrUqXK73Xr77bc1c+ZMLVy4UIsWLdLAgQP19a9/XZK0fv16nX322XK73crJydHPfvazsPDS0XV9XUTfTXNS4m4aAGifaUpNdfa8titJMozjnvbQQw/p008/1YQJE3TPPfdIkjIyMlRaWqoZM2bouuuu029+8xvV19frlltu0RVXXKE1a9aotLRUV111lX71q1/p0ksvVXV1td5++22ZpqnFixersLBQVVVVWrZsmSSpf//+7b7+7bffrpdffllLly7VqFGjtGHDBv3gBz9QRkaGZsyYETrvpz/9qR544AGNGDFC6enpkqSnn35aP/7xj/Xuu+/KNE3t379fc+fO1fz58/XMM89o586duu6665SQkKC77ror9FytrzsZEEbopgGA9jXVSffl2vPatx6Q4pOPe5rH41F8fLySkpKUnZ0d2r906VKdeeaZuu+++0L7/vjHPyovL0+ffvqpampq5PP5dNlll2no0KGSpIkTJ4bOTUxMVGNjY9hztlZbW6vf/OY3WrNmjfLz8yVJI0aM0DvvvKPf/e53YWHknnvuadOKMXLkSP3qV78Kbd92223Ky8vTo48+KsMwNGbMGB04cEC33HKL7rjjDjmav1m+9XUnA8JIMIz46qWmBsnFl0MBQF+3efNmrV27VikpKW2OffbZZ5o9e7ZmzZqliRMnas6cOZo9e7Yuv/xy9evXr8uvsWPHDjU0NLQJGV6vV5MnTw7bN3Xq1DbXt95XWFio/Px8GS1ahKZNm6aamhp98cUXGjJkSIfP1dcRRtxpkhEnmX7r9l5XxykYAE4priSrhcKu1z4BgUBAF198sX75y1+2OZaTk6O4uDitXr1a7733nlatWqVHHnlEt912mz744AMNHz68y68hSa+//roGDRoUdsztdodtJye3beVpvc80zbAgEtwnKWx/e8/V1xFGDENK8Ej1FVZXTSphBAAkWb8fu9BVYrf4+Hj5/f6wfWeeeaaWL1+uYcOGyels/6POMAxNmzZN06ZN0x133KGhQ4fqlVde0aJFi9p9ztbGjRsnt9ut4uLisC6Z7ho3bpyWL18eFkree+89paamtgk7JxvuppEYNwIAfdiwYcP0wQcfaO/evTp06JACgYCuv/56VVRU6KqrrtLGjRv1+eefa9WqVfrRj34kv9+vDz74QPfdd582bdqk4uJivfzyyzp48KDGjh0bes5t27Zp165dOnTokJqamtq8bmpqqhYvXqz//M//1NNPP63PPvtMW7Zs0W9/+1s9/fTTEb+P//iP/1BJSYluuOEG7dy5U6+99pruvPNOLVq0KDRe5GR1cr+7riKMAECftXjxYsXFxWncuHHKyMhQcXGxcnNz9e6778rv92vOnDmaMGGCbrrpJnk8HjkcDqWlpWnDhg2aO3euvvKVr+j222/X//zP/+iiiy6SJF133XUaPXq0pk6dqoyMDL377rvtvva9996rO+64QwUFBRo7dqzmzJmjv/zlL13u6mlp0KBBWrlypTZu3KhJkyZpwYIF+td//VfdfvvtJ1Q/fYFh9oH7gqqqquTxeFRZWam0tLSef4HnLpf2rJYueUya/P2ef34A6AMaGhpUVFSk4cOHKyGBwfzoms5+brr6+U3LiMRcIwAA2IgwItFNAwCAjQgjEmEEAAAbEUYkKSHdemw4amcpAAA4JRFGJFpGAACwEWFEIowAQAt94CZL9CI98fNCGJEIIwAgyeVySZLq6mz6pl70ScGfl+DPT3cwHbzU4tbeo3aWAgBsFRcXp/T0dJWXl0uSkpKS2nxXChBkmqbq6upUXl6u9PR0xcXFdfu5CCPSsZaRhkop4Jcc3a9QAOjLsrOt7+cKBhLgeNLT00M/N91FGJGO3U0j0wokSf3tLA0A2MYwDOXk5CgzM7Pd72MBWnK5XCfUIhJEGJEkZ7wUnyJ5a6xxI4QRAKe4uLi4HvmQAbqCAaxBzDUCAIAtCCNB3FEDAIAtCCNB3FEDAIAtCCNBtIwAAGALwkgQLSMAANiCMBJEywgAALYgjAQRRgAAsAVhJCg0C+tRW4sBAMCphjASFJxnhJYRAABiijASRDcNAAC2IIwEEUYAALAFYSSo5a29pmlnSQAAOKUQRoKCLSP+Rqmp3t6yAABwCiGMBMWnSI7mLzGmqwYAgJghjAQZBuNGAACwAWGkpeDtvcw1AgBAzBBGWqJlBACAmCOMtEQYAQAg5ggjLRFGAACIOcJISy3nGgEAADFBGGmJlhEAAGKOMNISYQQAgJgjjLQUDCPc2gsAQMwQRloKzjNCywgAADFDGGmJbhoAAGKOMNJSKIwctbUYAACcSggjLQXDSGOV5PfZWxYAAE4RhJGWEjzH1hsq7SsHAACnEMJIS3FOyZ1mrTNuBACAmCCMtBaahZUwAgBALBBGWgve3stcIwAAxARhpDVu7wUAIKYII60RRgAAiCnCSGuEEQAAYoow0lpoAOtRO0sBAMApgzDSGi0jAADEVERhpKCgQGeddZZSU1OVmZmpefPmadeuXce9bv369ZoyZYoSEhI0YsQIPf74490ucNQRRgAAiKmIwsj69et1/fXX6x//+IdWr14tn8+n2bNnq7a2tsNrioqKNHfuXF1wwQXasmWLbr31Vt14441avnz5CRc+KggjAADElDOSk//+97+HbS9btkyZmZnavHmzpk+f3u41jz/+uIYMGaIHH3xQkjR27Fht2rRJDzzwgL7zne90r9TRxDwjAADE1AmNGamstL6/pX///h2e8/7772v27Nlh++bMmaNNmzapqamp3WsaGxtVVVUVtsQMLSMAAMRUt8OIaZpatGiRzj//fE2YMKHD88rKypSVlRW2LysrSz6fT4cOHWr3moKCAnk8ntCSl5fX3WJGrmUYMc3YvS4AAKeoboeRhQsXatu2bfrTn/503HMNwwjbNps/5FvvD1qyZIkqKytDS0lJSXeLGblgGAn4JG9N7F4XAIBTVERjRoJuuOEGrVixQhs2bNDgwYM7PTc7O1tlZWVh+8rLy+V0OjVgwIB2r3G73XK73d0p2olzJUpx8ZLfa8014k61pxwAAJwiImoZMU1TCxcu1Msvv6w1a9Zo+PDhx70mPz9fq1evDtu3atUqTZ06VS6XK7LSxoJhMG4EAIAYiiiMXH/99Xruuef0wgsvKDU1VWVlZSorK1N9fX3onCVLluiaa64JbS9YsED79u3TokWLVFhYqD/+8Y/6wx/+oMWLF/fcu+hphBEAAGImojCydOlSVVZWaubMmcrJyQktL730Uuic0tJSFRcXh7aHDx+ulStXat26dTrjjDN077336uGHH+6dt/UGBcMIt/cCABB1EY0ZMbtwd8lTTz3VZt+MGTP00UcfRfJS9grONULLCAAAUcd307SHbhoAAGKGMNIewggAADFDGGlPYrr1WH/UzlIAAHBKIIy0h5YRAABihjDSHsIIAAAxQxhpD900AADEDGGkPQnMMwIAQKwQRtoTahmhmwYAgGgjjLQnOGbEWyP5vPaWBQCAkxxhpD0JHkmGtU5XDQAAUUUYaY8jTkpIs9YZxAoAQFQRRjrC7b0AAMQEYaQjhBEAAGKCMNIRwggAADFBGOlIQrr1yABWAACiijDSEVpGAACICcJIRwgjAADEBGGkI4QRAABigjDSEb4sDwCAmCCMdISWEQAAYoIw0hHCCAAAMUEY6Qi39gIAEBOEkY60bBkJBOwtCwAAJzHCSEeCA1jNgOSttrUoAACczAgjHXElSs5Ea51xIwAARA1hpDPc3gsAQNQRRjrDHTUAAEQdYaQzhBEAAKKOMNIZwggAAFFHGOkMc40AABB1hJHOhAaw0jICAEC0EEY6QzcNAABRRxjpTCiMHLW1GAAAnMwII51hnhEAAKKOMNIZumkAAIg6wkhnCCMAAEQdYaQzhBEAAKKOMNKZ4DwjvnqpqcHWogAAcLIijHTGnSYZzVXExGcAAEQFYaQzDsex1hG6agAAiArCyPFwey8AAFFFGDkeBrECABBVhJHjIYwAABBVhJHjIYwAABBVhJHjCQ5g5W4aAACigjByPLSMAAAQVYSR4yGMAAAQVYSR4yGMAAAQVYSR42GeEQAAooowcjy0jAAAEFWEkeMhjAAAEFWEkeMJhpGGSikQsLcsAACchAgjxxOcZ0Sm1FhpZ0kAADgpEUaOxxkvuZKtdbpqAADocYSRrmDcCAAAUUMY6QrCCAAAUUMY6QrmGgEAIGoII10RCiO0jAAA0NMII10R6qY5amsxAAA4GRFGuiJ4ey8tIwAA9DjCSFeEJj47amsxAAA4GRFGuoK7aQAAiBrCSFcQRgAAiBrCSFdway8AAFFDGOkKWkYAAIiaiMPIhg0bdPHFFys3N1eGYejVV1/t9Px169bJMIw2y86dO7tb5thrGUZM096yAABwkok4jNTW1mrSpEl69NFHI7pu165dKi0tDS2jRo2K9KXtEwwj/kapqd7esgAAcJJxRnrBRRddpIsuuijiF8rMzFR6enrE1/UK8SmSESeZfuv23vgku0sEAMBJI2ZjRiZPnqycnBzNmjVLa9eu7fTcxsZGVVVVhS22MgzGjQAAECVRDyM5OTl64okntHz5cr388ssaPXq0Zs2apQ0bNnR4TUFBgTweT2jJy8uLdjGPjzACAEBURNxNE6nRo0dr9OjRoe38/HyVlJTogQce0PTp09u9ZsmSJVq0aFFou6qqyv5AQhgBACAqbLm199xzz9Xu3bs7PO52u5WWlha22I65RgAAiApbwsiWLVuUk5Njx0t3Hy0jAABERcTdNDU1NdqzZ09ou6ioSFu3blX//v01ZMgQLVmyRPv379czzzwjSXrwwQc1bNgwjR8/Xl6vV88995yWL1+u5cuX99y7iAXCCAAAURFxGNm0aZMuvPDC0HZwbMe1116rp556SqWlpSouLg4d93q9Wrx4sfbv36/ExESNHz9er7/+uubOndsDxY8hwggAAFFhmGbvn1K0qqpKHo9HlZWV9o0f+cfj0t9vkcZfKn33KXvKAABAH9LVz2++m6araBkBACAqCCNdRRgBACAqCCNdFbq1lzACAEBPIox0VahlpNLecgAAcJIhjHRVMIw0Vkp+n71lAQDgJEIY6aqE9GPrDbSOAADQUwgjXRXnlOJTrfWGo7YWBQCAkwlhJBLcUQMAQI8jjESCO2oAAOhxhJFI0DICAECPI4xEItQyctTOUgAAcFIhjESClhEAAHocYSQShBEAAHocYSQShBEAAHocYSQSwYnPmGcEAIAeQxiJBC0jAAD0OMJIJAgjAAD0OMJIJAgjAAD0OMJIJFrOM2KadpYEAICTBmEkEsGWkUCT5K21tywAAJwkCCORcCVJcfHWOl01AAD0CMJIJAzj2O29hBEAAHoEYSRSwa4a5hoBAKBHEEYixR01AAD0KMJIpAgjAAD0KMJIpEK39xJGAADoCYSRSIVaRo7aWgwAAE4WhJFI0U0DAECPIoxEijACAECPIoxEKjjPCLf2AgDQIwgjkaJlBACAHkUYiRQDWAEA6FGEkUhxay8AAD2KMBKpYMuIt0byN9lbFgAATgKEkUgleI6t01UDAMAJI4xEyhF3LJDQVQMAwAkjjHRH8PZewggAACeMMNIdwXEjzDUCAMAJI4x0B3ONAADQYwgj3UEYAQCgxxBGuoO5RgAA6DGEke5gFlYAAHoMYaQ76KYBAKDHEEa6gzACAECPIYx0B/OMAADQYwgj3cE8IwAA9BjCSHfQTQMAQI8hjHRHy7tpTNPWogAA0NcRRrojOM+I6Zcaq20tCgAAfR1hpDtciZIzwVqnqwYAgBNCGOkuxo0AANAjCCPdRRgBAKBHEEa6KzjXCLf3AgBwQggj3UXLCAAAPYIw0l2EEQAAegRhpLuCt/cSRgAAOCGEke4KhZGjdpYCAIA+jzDSXXTTAADQIwgj3dVySngAANBthJHuCt7aS8sIAAAnhDDSXcGWEeYZAQDghBBGuisYRuoOSwG/vWUBAKAPI4x0l2ewlOCRfA1S8ft2lwYAgD6LMNJdcS5p7MXW+ifL7S0LAAB9GGHkREz4jvW44zXJ77O3LAAA9FERh5ENGzbo4osvVm5urgzD0Kuvvnrca9avX68pU6YoISFBI0aM0OOPP96dsvY+w6ZLSQOtcSNF6+0uDQAAfVLEYaS2tlaTJk3So48+2qXzi4qKNHfuXF1wwQXasmWLbr31Vt14441avvwk6NqIc0rjLrHWP3nZ3rIAANBHGaZpmt2+2DD0yiuvaN68eR2ec8stt2jFihUqLCwM7VuwYIH++c9/6v33uzbws6qqSh6PR5WVlUpLS+tucaNj77vSU3Mlt0f6yW7J6ba7RAAA9Apd/fyO+piR999/X7Nnzw7bN2fOHG3atElNTU3tXtPY2Kiqqqqwpdcaki+l5kiNldKet+wuDQAAfU7Uw0hZWZmysrLC9mVlZcnn8+nQoUPtXlNQUCCPxxNa8vLyol3M7nM4pPGXWuvb6aoBACBSMbmbxjCMsO1gz1Dr/UFLlixRZWVlaCkpKYl6GU9I8K6anSslb529ZQEAoI+JehjJzs5WWVlZ2L7y8nI5nU4NGDCg3WvcbrfS0tLCll5t0BQpfYjUVCvtfsPu0gAA0KdEPYzk5+dr9erVYftWrVqlqVOnyuVyRfvlY8MwjrWOMAEaAAARiTiM1NTUaOvWrdq6dask69bdrVu3qri4WJLVxXLNNdeEzl+wYIH27dunRYsWqbCwUH/84x/1hz/8QYsXL+6Zd9BbBMPIp6ukhl484BYAgF4m4jCyadMmTZ48WZMnT5YkLVq0SJMnT9Ydd9whSSotLQ0FE0kaPny4Vq5cqXXr1umMM87Qvffeq4cffljf+c53eugt9BJZE6QBoyR/o7Trb3aXBgCAPuOE5hmJlV49z0hLawuk9fdLo+ZI3/8/u0sDAICtes08I6eUCZdZj5+9JdVV2FsWAAD6CMJIT8oYLWVNlAI+qfAvdpcGAIA+gTDS0yYwARoAAJEgjPS08c1dNUUbpJpye8sCAEAfQBjpaf2HW5OgmQFpx2t2lwYAgF6PMBINTIAGAECXEUaiYdw867H4fanyC1uLAgBAb0cYiQbPIGnIedb69ldtLQoAAL0dYSRagnOO0FUDAECnCCPRMm6eZDikAx9JFZ/bXRoAAHotwki0pGRIw2dY658w5wgAAB0hjERTqKuGMAIAQEcII9E05luSwyWVb5fKd9pdGgAAeiXCSDQl9ZdGzrLWmR4eAIB2EUaireUEaKZpb1kAAOiFCCPRNvoiyZkgHd4jlW2zuzQAAPQ6hJFoc6dKo2Zb6wxkBQCgDcJILIS6al6mqwYAgFYII7EwarYUnyJVFktfbLK7NAAA9CqEkViIT5JGz7XWmR4eAIAwhJFYCU6Atv0VKeC3tywAAPQihJFYOe2rUoJHqimTit+3uzQAAPQahJFYcbqlsRdb63TVAAAQQhiJpeBdNTtek/xN9pYFAIBegjASS8OmS0kDpbrDUtF6u0sDAECvQBiJpTinNO4Sa50J0AAAkEQYib1gV03hXyVfo71lAQCgFyCMxNqQfCk1R2qslPa8ZXdpAACwHWEk1hwOaXzznCPcVQMAAGHEFsEJ0Hb9TfLW2VsWAABsRhixw6ApUvoQqanWus0XAIBTGGHEDoYhTbrKWv/rf0pFb9tbHgAAbEQYscv5i6SRX5d89dILV0h737W7RAAA2IIwYhdXgnTlc9Jps6SmOun570r73rO7VAAAxBxhxE6uBOl7L0gjLrTGjzz/Xan4H3aXCgCAmCKM2M2VIF31J2nETMlbIz33Han4A7tLBQBAzBBGegNXovS9P0nDpx8LJCUf2l0qAABigjDSW8QnSVe9KA27QPJWS89dJn2x2e5SAQAQdYSR3iQ+Wbr6JWnoNKmxSnr2Umn/R3aXCgCAqCKM9DbxydLV/ycNOc/6/ppn50kHtthdKgAAooYw0hu5U6Tv/5+Ud67UUCk9M086sNXuUgEAEBWEkd7KnSr94M/S4LOlhqPSM5dIpdvsLhUAAD2OMNKbuVOlHyyXBp/VHEi+LZV9bHepAADoUYSR3i4hzQokg6ZI9Uekp78tfbnd7lIBANBjCCN9QYJH+sHLUu5kqb5Cevpi6csddpcKAIAeQRjpKxLTpR++IuWcIdUdtgJJeaHdpQIA4IQ57S4AIpDYzwokz1wilW2THj9fyp4o5Z1zbPEMsruUAABExDBN07S7EMdTVVUlj8ejyspKpaWl2V0c+9VVSH+6Sipp50v1PHlS3tnWbcF5Z0tZE6Q4MicAIPa6+vlNGOnLKr+wvuW35ANrKftEMv3h57iSpcFTmltOzpUGT7W6fAAAiDLCyKmosUbav/lYOCn50JrFNYwhZY61QkmCRzLiJEdcq0dH+LbhaF53HNuX4JGyxkv9hlvnAwDQSlc/v2m/P5m4U6QRM6xFkgIB6eBOqzunZKPVinKkSCrfYS09wZUsZY2zuoOyJ0hZE61td2rPPD8A4KRHy8ippqbcajUp/afUVC+ZAWsJ+K0untBjoNW2P/zcmi+tu3n8je2/Tr/hx8JJ9gQrrKQPkQwjtu8XAGAbumkQfX6fdHiP9OUn1lLW/Fhd2v757uaunazxUr9hUmq2lJojpeVIKdlSfFJMi3+yM01Tjb6Aahp9qm30yRnn0MCUeLmdcXYXDcApgjAC+9Qelr78+Fg4KfvE6i4KNHV+XYLHCiep2VJq7rGwkpotpTVvp2RJca7YvI9epLK+SW/vPqijdU2qbfSppnk5tu5XTUOTahv9Ycd8gbb/vdMSnMpIdSsj1a2BKeGPGS22B6TEyxXHeCAA3UcYQe/i80qHPrWmsi/fLlXul6rLrFaU6lKpqa6LT2RY860kpEnuNCvAuFOtdXdq8/7gdtqx88KOefrMoNsmf0DP/2OfHnprt47UHSfMdSIpPk5N/oCa/JH9d++X5FJGqluZqQk6LSNZo7PTNDo7VaOzU5XiZsgZgM4RRtB3mKbUWGWFk6oDLUJKmVQd3G7eF/D1wAsa1u3Nif2tYJPU/JjYv8V6v7bH3KmxGfMSCMhsqNR7H3+qP63bovrKcvU3qjUk2a80Tz+ZzSHMSPAoLildziSPXMnpSk5MUorbqWR3nFITnEp2Ny/xTsU5DJmmqcr6Jh2sbtTBmkYdrG7UoRpv82Nj2OPhWq/87bSqtJTXP1Gjs9I0pjmcjM1J1bAByXLSmgKgGWEEJ59AwJoKv/agFV4aq6WGSusxtF3V/rGG5n0dDbjtCodTSki3xrbEp0iuJGvdldy8L/nYuqt5Oz752LorSfI1Wu+hzVJhPdZXyKyrkNF6vpiucCZarT8JnmOtQqF1jxWqkgdKSQOtx+B6OyErEDB1pM4bCiullfXaXV6jnWXV2llapfLq9usx3unQyIyUUEAZk2OFlcxUtwwGLwOnHMJIDHx+sEZ1Xr/G5aTJ4eAX7fF8caRO6z89qA8+r5Db6VC2J8Fa0o499k+Oj+6HVlODFVLqK6wAUH/EWq8/Er5dd6TFesWJhZhuqjET5Uvop9T+WYpLbg4N3trmkFVlPTZUSd7qE3uhuHgpOUNKGtAirGRIyQOOBZekAdZYHcOag6aqwa/PKxpUdKhOn1fUa8/Beu05VK86b0B+ORQILYb8csiT4NBp/eN1Wj+XhqY7NSQtToPT4pSbYijR8El+rxXUQo+NVteev1HyN1mtZ4ZDMiTJaF43wtcNR/N2O+tOtxTntt6D02295+Bjy/U2+9x9pksP6K7aRp92fVmtkZkpSkvo2TF5zDMSZaWV9Zr78NtqaAooM9Wtr47J1NfGZmnayIFKjO/ddyv4/AF9VHxUnkSXTsuIXrN6o8+vD4uOaN2ucq379KD2lNcc95p4p8MKJ8GA4mm7npnq7n6ZXQnWkpoV2XXeuuZwcsQa3+KtbX6sk5pqre3QeovH0HnNj0639cHeYmmMT9eqvT79eUedvvQlq8JM1fkTR2nR3Ika3K8LdxgF/Mdaf1oHlZbbdRVS3SGp9tCxx6Y6KwBU7beWLkqTdEbzEuKQlNDJRRXNS59jWK1ijjjrMTghYNg+R/N2cF+L/d0V+jvRbGfdDG0eWzdbnSsrfAXDVbvrro73y2h+vhavawbCy2AGWr1u8BxDcjb/X3MlNa8ntb/d+lhcfM90h5otytxyUet9ZgfrrZcOjqnV9VJz3alFeDZaPLa3L3i+0Wq6hc7K5G9btoCveWmejiG0bS1+v09Hqut0sKpWh6rqVFFdpyPV9aptaFCcAvJ98990dv6ME6/7biCMdNOLG0vU0BSQJJVXN+rFD0v04oclcjsdOn/kQM0am6VZYzOVldbZb+fYKz5cp5te2qItxUclSQkuh8bmpGniII8m5Ho0YZBHo7JSun0XRUlFndZ9elDrd5Xrvc8Oq857rLvBYUhnDumnC0ZlKM4hlVU1qKyyIfR4qMYrry+g4oo6FVd0PKDV6TA0uF+ihg1M1vDmZdgA6zE3PVFx0Wilim/ukunBLyL0B0wt3/yFHli1K9TtMXVoPxV8c6wmD+nX9SdyxB0b4xIpb92xYNIypLTeV1dxbM6ZsF+Uwe1WvyCDx3XsgzFgOOU3XPIaLjWaTtUHnGoIxMkrlxrllFcuec3mR7nUJKec8QlKSEiQ3zTk8/vl8/nV5PfL57d+ATsM6wPQkJrbYSRDZtjiVEAu+RSvJrkMv9xqktvwKTnOr0SHT27DL5d8cpleOQJNcgS8rSrJtO4EO97dYOghRvgdc20a71ttt3PcNK1/e4SLkzSweQnTnAQ2VcyUZE8YoZumG3z+gM7/5VqVVTXo15efrqy0BL1V+KXeLCzX/qP1YedOHOTRrLFWq8n43DRb+81f2fKFfv7qdtU0+pQUHydDUq237diEeKdDY7JTNaE5oEwc5NFXslPanZ+i0efXxqIKrdt1UOt2leuzg7VhxzNS3ZrxlQzNHJ2hC0ZmyJPUcRNgo8+v8qrGYyGlRVAJPn5Z1dDu7aqhssc5lNc/8VhIGZis4QOsx+y0hF7TnfbunkP6xeuFKiytkiQN6Z+kn100RhdNyD65xlaE/pqWFZpaqaxv0t5DtSo6VKvPmx+LDtWo6GBtuz+brbniDHkSXUpLcCk10aW0BKfSmreD6/6AqZKKOpUcqVNJRb0OVNa3/fwKL7Sykx0alu7S8HSnctJcykpxKiPZqYwUlzKT49Q/0SGXYbb9K9T0h+/rztifMC3+Ym7zV7TU4V/bQYEmq5vL723x2N5663Mam7vGWnWFhf1F32pfsBXIMKxrfY2Sr96aXLGpwWqF8zU0b9e3PXbCdXXiAqYVYfxyyGyOt9bikBlab71tnWc2R2HDMGQYVkOhwzDlkJq3TeuYTDkMKzRbx4JXSpIp04iTDOuoaTQvcsg0rMhtymjeZ8g04mQGz5VU7zdU2yRVe6UGvyG/4uSTI+xRhlPJiW6lJbmVmpQoT0qi+qUkKiE+Xjr9CilnUo/WKWNGoujNHV/q/3tmk/olufSPW2eFPqRN09SuL6v1VmG53iz8UltLjob90stOS9BXx2bq62OzlH/aACW4YtOdU9XQpJ+/+ole23pAknTWsH763yvPUK4nUUWHa/XJ/srmpUqfHKhUdUPbO1ZccYa+kpVqtZ4M9kimqXW7Duq9zw6rvunYL5E4h6EpQ/ppxmgrgIzL6dkAFgiY+rK6QUWHarX3UJ32Hq7V5wdrtfdwrYoP18nrD3R4bYLLoaH9kzV0QJKGDkjSkAHJGtI/SUP7J2lQv8SYzKmxp7xGBSsL9dbOcknWnB83zhqlH+YPZTKyFkzT1MHqRn1+qFYlFXWKdzpCIcOT6LTCRqJLbqcj4p8vry+gA0frVdwcUIor6qywUmHtq6w/fguIYUgDkt3K9riVlZqgrGB3Ytqx9aw0tzyJrpMrXPawmkaf3tl9SOs/Ldc7O0t1tKpKCWpUnNr/f2yqbV12tC8gQ4bDIbfLJbfLqYR4p9yuOCXEu5oXp/Xosh6T3E4lxVtNBPVev+q8ftU3+VTbGL5e7/WrrsmnutB++0NUexyGNGxgsjWYPCtNY3JSNSY7VXn9kmL6RxlhJIp+9NSHWrOzXNddMFy3fXNch+cdrG7U2l3leqvwS729+1BYl0WiK04XjBqo7587VNNHDYzaL6xNeyt080tb9cWResU5DN00a5T+Y+ZpHY65ME1TxRV1+mR/lT7eX6ntByr18f5KHe1kjovMVLdmjs7QzNGZmjZyoDyJ9kxK5g+YOnC0XnsPB//CrtXeQ7Xae9j6sOmsRcVhSLnpiVZIaQ4sQ/pby9ABSUrtZFCXaZqq9fpVVd+kyvomVdU3qarB1/zYpKp6n6oamlRW2aC/by+TP2DK6TD0g3OH6qZZo9QvOT4a1YFuqqxvag4nVlgpbW6Rs1rmGlVe3dDl+VoSXXHKTU9QbnqiBvdL1KD0RA3ql6hB6VYAzjqR8U99kGma+vTLGmsc2a6D2rSvIqwu3U6Hzh0xQBMGpcntjFO806H4OIf16HTI3Wq79TFX83aSy6nE+Di54oyoh8FAwFR9kxVM6rw+1TdZgaW+ya+GJr/qvQFrX5Nf9V5faLuhxXnBa7x+q6vREWxdaX60WlSs31PB9VALjCEZMuRwSLmeROsutuw0jcpKidkfvJ0hjETJ/qP1uuCXaxQwpTX/NUMjMlK6dF1Dk1/vf35YbxV+qbcKy1Va2RA6NnGQR9dfeJpmj8vuscTq8wf0yJo9emTNbgVMa06IB6+crClDIx9XYJqm9h+t1yf7q0LhpMkf0LSRAzXzK5kam5Pa6//6a/IH9MWReu09VKt9h2tVXFGv4opa7Tts/WXc6Ou4RUWyJv8aMiBZmalu1Tb6wkJGVX2TjjMlR5ivj8vSkovGdPlnB71LIGCqos4b6jYsq7JCypeVwXXrsbMAHxTnMJSdlqBB/RI1OBRUEpXbvD4wxa1Ut7PXdC92R3VDk97dc1jrPy3X+l0HdaDF7z5JGjYgSTNHZ2rG6Azlj4hdizFigzASJb9Z/akefmu3zh3RXy/+W363nsM0Te0ordLyzfv1p43FoWa+kZkp+o+Zp+nbk3JP6K+lkoo63fzSVm3ed0SSdNnkQbr7kvGd/nV/KgsETB2saQwFk+LDtdpXEVyv0+Ha1gMa29fp+IXmroVzhvfX1GH9o/yO0Bs0NPlVVtmg/Ufrtf9Ivb5oftx/tE77j9ar9Gjn45+CDENKdYf/LHlC6y26roLrSdZjsjvYfazQoymz+dH6PWSGjh/bH2jxkWCo+S/zFmVRi33H/ko/dq5kdQ2/s/uQ1u4q16a9R8Lep9vpUP5pAzTzK1Zr6rCBySdSzejlohpGHnvsMf36179WaWmpxo8frwcffFAXXHBBu+euW7dOF154YZv9hYWFGjNmTJder7eEEZ8/oGm/XKMvqxr18FWT9e1JuSf8nIdrGrXs3b16+v29obEaef0TtWDGafrOmYMj/ivhta37dfsrn6i60adUt1O/uHSCLjmj5+4AORVVNzQ1jyeo1aEar1ITnGEBw5PY/fELOHX5A6bKqxt04Gi9vjhSHwotLR/rujCIty8Itn7MHJ2hc2n9OKVELYy89NJL+uEPf6jHHntM06ZN0+9+9zs9+eST2rFjh4YMGdLm/GAY2bVrV1hBMjIyFBfXtR/I3hJGVm0v0789u1n9k+P1/pKv9uiAw6qGJj37/j798Z2i0F/imalu/dv0Ebrq7CFKPs73gFQ3NOmO17brlS3WXBFThvbTg1eeobz+fBMu0Fc1NPnbdAlWthyT1GpcUssxSzWN1h83LccgtGzNUMvtFuuO4MEOWlGC62rvWPM18XEOTRnWj9YPRC+MnHPOOTrzzDO1dOnS0L6xY8dq3rx5KigoaHN+MIwcOXJE6enpkbxUSG8JI/+ybKPW7jqof58+Qkvmjo3Ka9R7/Xrxw2I9seHz0LiSfkku/cu04bo2f1i7t8Zu3ndEN7+0RSUV9XIY0o2zRmnhhSNPqYFxAIDep6uf3xF9Wnm9Xm3evFmzZ88O2z979my99957nV47efJk5eTkaNasWVq7dm2n5zY2NqqqqipssdsXR6zJvCTpe2e3bQHqKYnxcfqXacO1/icX6pffmahhA5J0pK5Jv1n9qab9co3u/9tOHWyeIMsfMPXwW7t1xe/eV0lFvQb3S9T/W5Cvm7/2FYIIAKDPiGgG1kOHDsnv9ysrK3wq7aysLJWVlbV7TU5Ojp544glNmTJFjY2NevbZZzVr1iytW7dO06dPb/eagoIC3X333ZEULepe+rBEpimdd9oADY9Bk2O806Erzxqi75w5WCs/KdNja/doZ1m1Hl//mZa9W6Qrz8pTYWmVPtxrDVKdd0au7pk3oce/VwAAgGjr1nTwrQfpmabZ4cC90aNHa/To0aHt/Px8lZSU6IEHHugwjCxZskSLFi0KbVdVVSkvL687Re0RPn9AL31YIkm6+pzotYq0xxnn0Lcn5epbE3O0Zme5Hl27R1tLjuqZ9/dJklLcTt07b7wunTw4puUCAKCnRBRGBg4cqLi4uDatIOXl5W1aSzpz7rnn6rnnnuvwuNvtltvtjqRoUfXWznKVVzdqQHK8Zo/LtqUMDoehr42zvu/m/c8O63cbPpck3XvJBA0ZwCBVAEDfFVEYiY+P15QpU7R69Wpdeumlof2rV6/WJZdc0uXn2bJli3JyciJ5aVu98EGxJOnyqYMV77R3LIZhGDpv5ECdN7LNVx0BANAnRdxNs2jRIv3whz/U1KlTlZ+fryeeeELFxcVasGCBJKuLZf/+/XrmmWckSQ8++KCGDRum8ePHy+v16rnnntPy5cu1fPnynn0nUVJSUacNu62Bq1edFdsuGgAATgURh5Err7xShw8f1j333KPS0lJNmDBBK1eu1NChQyVJpaWlKi4uDp3v9Xq1ePFi7d+/X4mJiRo/frxef/11zZ07t+feRRQFB65OGzmAe+UBAIgCpoPvRJM/oPPuX6OD1Y367dVn6pun952uJQAA7BaVeUZONW8VlutgdaMGpsTr6+O6PkAXAAB0HWGkEy9sbB64OiXP9oGrAACcrPiE7UBJRZ3eDg5cPdu+OU4AADjZEUY68OKHxTJN6YJRAzV0AANXAQCIFsJIO5r8Af3fpi8kSVdH8XtoAAAAYaRdb+74snngqltfY+AqAABRRRhpR3Dg6hVTB8vFt98CABBVfNK2Uny4Tm/vPiRJuoouGgAAoo4w0sqfPrRaRS4YNVB5/fkCOgAAoo0w0oLXF9D/21QiSfr+ObSKAAAQC4SRFt4s/FKHarzKSHVr1lgGrgIAEAuEkRZe+ICBqwAAxBqfuM32Ha7VO3sOyTCk751FFw0AALFCGGn2p43WWJHpozIYuAoAQAwRRmQNXP3zZiuMXM3AVQAAYoowImn1DmvgamaqW18dk2l3cQAAOKUQRiS9sHGfJOnKs/IYuAoAQIyd8p+8ew/V6t09h2UYVhgBAACxdcqHkeCMqzO+kqHB/Ri4CgBArJ3SYcTrC+jPm76QJF3N99AAAGCLUzqMvLG9TIdrvcpKY+AqAAB2OaXDyJ82Wl00V07Nk5OBqwAA2MJpdwHsdNOsUcpIdetKumgAALDNKR1GzhkxQOeMGGB3MQAAOKXRNwEAAGxFGAEAALYijAAAAFsRRgAAgK0IIwAAwFaEEQAAYCvCCAAAsBVhBAAA2IowAgAAbEUYAQAAtiKMAAAAWxFGAACArQgjAADAVn3iW3tN05QkVVVV2VwSAADQVcHP7eDneEf6RBiprq6WJOXl5dlcEgAAEKnq6mp5PJ4Ojxvm8eJKLxAIBHTgwAGlpqbKMIywY1VVVcrLy1NJSYnS0tJsKmHfQ71FjjrrHuqte6i37qHeIhfNOjNNU9XV1crNzZXD0fHIkD7RMuJwODR48OBOz0lLS+MHrxuot8hRZ91DvXUP9dY91FvkolVnnbWIBDGAFQAA2IowAgAAbNXnw4jb7dadd94pt9ttd1H6FOotctRZ91Bv3UO9dQ/1FrneUGd9YgArAAA4efX5lhEAANC3EUYAAICtCCMAAMBWhBEAAGCrPh1GHnvsMQ0fPlwJCQmaMmWK3n77bbuLZKsNGzbo4osvVm5urgzD0Kuvvhp23DRN3XXXXcrNzVViYqJmzpyp7du3h53T2NioG264QQMHDlRycrK+/e1v64svvojhu4itgoICnXXWWUpNTVVmZqbmzZunXbt2hZ1DvbW1dOlSnX766aFJkvLz8/W3v/0tdJw6O76CggIZhqGbb745tI96a+uuu+6SYRhhS3Z2dug4ddax/fv36wc/+IEGDBigpKQknXHGGdq8eXPoeK+qO7OPevHFF02Xy2X+/ve/N3fs2GHedNNNZnJysrlv3z67i2ablStXmrfddpu5fPlyU5L5yiuvhB2///77zdTUVHP58uXmxx9/bF555ZVmTk6OWVVVFTpnwYIF5qBBg8zVq1ebH330kXnhhReakyZNMn0+X4zfTWzMmTPHXLZsmfnJJ5+YW7duNb/5zW+aQ4YMMWtqakLnUG9trVixwnz99dfNXbt2mbt27TJvvfVW0+VymZ988olpmtTZ8WzcuNEcNmyYefrpp5s33XRTaD/11tadd95pjh8/3iwtLQ0t5eXloePUWfsqKirMoUOHmvPnzzc/+OADs6ioyHzzzTfNPXv2hM7pTXXXZ8PI2WefbS5YsCBs35gxY8yf/exnNpWod2kdRgKBgJmdnW3ef//9oX0NDQ2mx+MxH3/8cdM0TfPo0aOmy+UyX3zxxdA5+/fvNx0Oh/n3v/89ZmW3U3l5uSnJXL9+vWma1Fsk+vXrZz755JPU2XFUV1ebo0aNMlevXm3OmDEjFEaot/bdeeed5qRJk9o9Rp117JZbbjHPP//8Do/3trrrk900Xq9Xmzdv1uzZs8P2z549W++9955NperdioqKVFZWFlZnbrdbM2bMCNXZ5s2b1dTUFHZObm6uJkyYcMrUa2VlpSSpf//+kqi3rvD7/XrxxRdVW1ur/Px86uw4rr/+en3zm9/U1772tbD91FvHdu/erdzcXA0fPlzf+9739Pnnn0uizjqzYsUKTZ06Vd/97neVmZmpyZMn6/e//33oeG+ruz4ZRg4dOiS/36+srKyw/VlZWSorK7OpVL1bsF46q7OysjLFx8erX79+HZ5zMjNNU4sWLdL555+vCRMmSKLeOvPxxx8rJSVFbrdbCxYs0CuvvKJx48ZRZ5148cUX9dFHH6mgoKDNMeqtfeecc46eeeYZvfHGG/r973+vsrIynXfeeTp8+DB11onPP/9cS5cu1ahRo/TGG29owYIFuvHGG/XMM89I6n0/b33iW3s7YhhG2LZpmm32IVx36uxUqdeFCxdq27Zteuedd9oco97aGj16tLZu3aqjR49q+fLluvbaa7V+/frQceosXElJiW666SatWrVKCQkJHZ5HvYW76KKLQusTJ05Ufn6+TjvtND399NM699xzJVFn7QkEApo6daruu+8+SdLkyZO1fft2LV26VNdcc03ovN5Sd32yZWTgwIGKi4trk8zKy8vbpDxYgqPPO6uz7Oxseb1eHTlypMNzTlY33HCDVqxYobVr12rw4MGh/dRbx+Lj4zVy5EhNnTpVBQUFmjRpkh566CHqrAObN29WeXm5pkyZIqfTKafTqfXr1+vhhx+W0+kMvW/qrXPJycmaOHGidu/ezc9aJ3JycjRu3LiwfWPHjlVxcbGk3ve7rU+Gkfj4eE2ZMkWrV68O27969Wqdd955NpWqdxs+fLiys7PD6szr9Wr9+vWhOpsyZYpcLlfYOaWlpfrkk09O2no1TVMLFy7Uyy+/rDVr1mj48OFhx6m3rjNNU42NjdRZB2bNmqWPP/5YW7duDS1Tp07V97//fW3dulUjRoyg3rqgsbFRhYWFysnJ4WetE9OmTWszTcGnn36qoUOHSuqFv9t6dDhsDAVv7f3DH/5g7tixw7z55pvN5ORkc+/evXYXzTbV1dXmli1bzC1btpiSzN/85jfmli1bQrc733///abH4zFffvll8+OPPzavuuqqdm/jGjx4sPnmm2+aH330kfnVr371pL4F7sc//rHp8XjMdevWhd06WFdXFzqHemtryZIl5oYNG8yioiJz27Zt5q233mo6HA5z1apVpmlSZ13V8m4a06Te2vNf//Vf5rp168zPP//c/Mc//mF+61vfMlNTU0O/66mz9m3cuNF0Op3mf//3f5u7d+82n3/+eTMpKcl87rnnQuf0prrrs2HENE3zt7/9rTl06FAzPj7ePPPMM0O3Y56q1q5da0pqs1x77bWmaVq3ct15551mdna26Xa7zenTp5sff/xx2HPU19ebCxcuNPv3728mJiaa3/rWt8zi4mIb3k1stFdfksxly5aFzqHe2vrRj34U+r+XkZFhzpo1KxRETJM666rWYYR6ays494XL5TJzc3PNyy67zNy+fXvoOHXWsb/85S/mhAkTTLfbbY4ZM8Z84oknwo73prozTNM0e7atBQAAoOv65JgRAABw8iCMAAAAWxFGAACArQgjAADAVoQRAABgK8IIAACwFWEEAADYijACAABsRRgBAAC2IowAAABbEUYAAICtCCMAAMBW/z+g62XJbxGbwQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# on the set used to train\n",
    "training_errors = []\n",
    "# on the full test set\n",
    "test_errors = []\n",
    "\n",
    "sizes = [size for size in range(20, 620, 20)]\n",
    "\n",
    "# returns the vector of weights from least squares regression\n",
    "def train(X, y):\n",
    "    return np.linalg.inv(X.transpose() @ X) @ X.transpose() @ y\n",
    "\n",
    "for size in sizes:\n",
    "    # get the size of data we want to work with\n",
    "    features = std_X_train_wbias[:size, :]\n",
    "    response = y_train[:size]\n",
    "\n",
    "    # calculate the best weights\n",
    "    w = train(features, response)\n",
    "    # print(w)\n",
    "    # find the training error\n",
    "    y_train_pred = features @ w\n",
    "    training_errors.append(mse(y_train_pred, response))\n",
    "\n",
    "    # find the test error\n",
    "    y_test_pred = std_X_test_wbias @ w\n",
    "    # print(y_test_pred)\n",
    "    test_errors.append(mse(y_test_pred, y_test))\n",
    "\n",
    "    # print(features.shape, response.shape)\n",
    "\n",
    "# plot both train and test learning curves\n",
    "plt.plot(sizes, training_errors, label = \"training error\")\n",
    "plt.plot(sizes, test_errors, label = \"test error\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# print(training_errors)\n",
    "# print(test_errors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Qs:\n",
    "> explain whether you think the model is underfitting or not, and how much data you need before getting the optimal test error.\n",
    "\n",
    "The model is definitely not overfitting because test error is staying down with train error. I don't think the model is underfitting either because it isn't doing worse on more data, it appears to have plateaued at a min-test MSE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
